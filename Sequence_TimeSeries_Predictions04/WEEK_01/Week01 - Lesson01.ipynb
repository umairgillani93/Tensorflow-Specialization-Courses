{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Time-Series:\n",
    "\n",
    "Time Series is making predictions off of the __Data__ which is in sequential form. For instance, we want to predict a __Stalk Prices__ and __Weather__ based on previous time values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Time-Series:\n",
    "\n",
    "Time Series could be off many types. Some of them follow some sort of pattern and some are completely random. \n",
    "Let's dicsuss a few of those.\n",
    "\n",
    "* __Trend Following__: In this time-series we have data points that follow some sort of __Trend__. For instance an inclinde straight line. \n",
    "\n",
    "\n",
    "* __Sessionality__: It's the time-series which follow some sort of repeated intervels. For instance, consider the example of passengers travelling to different countries - During vacations session, the flights increse and vice-versa following some sessional condition. \n",
    "\n",
    "\n",
    "* __Auto-correlation__: In this type of time-series we don't have any __Trend__ or __Sessionality__ but we still have some sort of pattern following. For examples, during each raising spike we have some lagging series of values. It Auto-correlates with the delayed copy of itself. We can have __Multiple__ Auto-correlations in a single time-series i.e Series of lagging factors and differenct thresholds following bike a spike\n",
    "\n",
    "* __Non-Stationary__: This type of time-series follows exact same pattern for some specific __Threshold__ but after that becomes completely random. Mostly in real-life, we have this type of Time-Series available for our analysis \n",
    "\n",
    "__Time-Series__ we have in our daily lives, follows a bit of each such pattern i.e __Trend + Sessionality + Auto-Correlation + Noise__. Off of that data, we can spot the patterns, and following __Machine Learning__ on this sort of sequential data, we can come up with future predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputed-Data:\n",
    "\n",
    "ormally we have values missing in our sequential data. This can decrease the performance of our model. So, what we can do is to __Impute__ the data, i.e fill-up those empty values with some sort of data. This data is taken from previous values that what pattern our data followed, and on the basis of that we impute the gaps with some __mean/average__ values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Validation and  Test Sets:\n",
    "\n",
    "* __Fixed Partioning__: When dealing with __Sequential Data__ we have this problem of splitting data into __Train__ and __Validation__ sets. Becauses __Validation__ set grabs random samples from our entire dataset, causing gap between time-series intervels. For this we have a __Fixed Partioning__ strategy to resolve this problem. We just Pationed the data on fixed interverls for __Training__, __Validation__ and __Testing__, followed by a sequence. \n",
    "\n",
    "\n",
    "* __Roll-Forward Partioning__: In this __Training__ paradigm we start with fixed training partition and gradually increase or __Roll-Forward__ along the time-series. For each iteration we increase the size of the traning data, and train it on few more samples. In this way, it continues on to refine the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model:\n",
    "\n",
    "__Errors = Predicted - Actual__\n",
    "\n",
    "* Mean Squared Error -> mse = np.square(error).mean() -> Better for penalizing large errors\n",
    "* Root Mean Squared Error -> rmse = np.sqrt(mse)\n",
    "* Mean Absolute Error -> mae = np.abs(error).mean()\n",
    "* Mean Absolute Percentage Error -> mape = np.abs(errors / x_valid).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving Average:\n",
    "\n",
    "The average plot of data points over a specific interval is called __Moving Average__. It can help reduce the noise of our data. Forcastings should not be made on __Moving Averages__ as it perform even worst then __Naive Forcasting__ (forcasting considering the last bunch of values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
